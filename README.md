# Schema Naming Assessments for Improved LLM-Based SQL Inference
This repository contains datasets, code, and analysis described in the paper: SNAILS: Schema Naming Assessments for Improved LLM Systems.

## What is SNAILS?
SNAILS is a set of NL-to-SQL-related artifacts designed to explore the effects of schema naming on LLM-based NL-to-SQL performance.
It includes a set of real-world databases and associated natural language (NL) : SQL gold query pairs, human-labeled data containing naturalness classifications the tables and columns in the database collection, a model and method for ML-based naturalness classification, and prompting strategies for improving schema identifier naturalness.

We used these artifacts to explore the true effect of schema identifier (tables and columns) names on NL-to-SQL performance, and present them in our SIGMOD 2025 paper. Our results can be reproduced using the artifacts in this repository using the instructions described below.

## What SNAILS isn't:
SNAILS is not a fully-featured benchmark similar to [Spider](https://yale-lily.github.io/spider) or [BIRD](https://bird-bench.github.io/) that maintains a leaderboard, and provides a training, and dev dataset while also retaining a hidden test set. Instead SNAILS artifacts are a starting point for further research into schema-specific effects on NL-to-SQL. It can also be used to support the development of new NL-to-SQL methods. We encourage other researchers and developers to use our artifacts for their research objectives, and invite collaboration and feedback from the community.

## SNAILS Artifacts
For ease of access, we make the artifacts referenced in our paper available in the following directory tree:

```
SNAILS\
|___SNAILS_Artifacts\
   |__databases\ # Contains .txt files with download links and instructions
   |__naturalness_classifications\ # .xlsx and .csv files with all SNAILS identifiers and naturalness scores
   |__naturalness_classifiers\ # ML Model and prompts for naturalness classification
   |__naturalness_modified_identifiers\ # crosswalks (or maps) of native to modified naturalness identifiers
   |__naturalness_modifier\ # RAG-based identifier renamer (see README.md here for more detail)
   |__nl_sql_question_query_pairs\ # 503 NL and gold query pairs for SNAILS evaluation 
```

## Results Reproducibility
The workflows for generating SNAILS results are somewhat complex and require multiple steps which we describe below in the **SNAILS Project Setup** section.
You can reproduce the entire process, including LLM-based SQL generation, or you can use the SQL which the LLMs already generated to recreate the data that led to our findings. Scroll down to the  section to get started.
The root folder contains Jupyter notebooks to guide the reproduction process and should be executed in the proper order:

```
##### Core Findings #####
|__01-SNAILS-NL-to-SQL-Inference-and-Scoring.ipynb - SQL Inference over SNAILS collection
|__02-SNAILS-NL-to-SQL-results-analysis.ipynb      - Performance metric creation (e.g., QueryRecall, Execution Accuracy)
|__03-SNAILS-identifier-analysis.ipynb             - Identifier-focused metrics (e.g., Identifier Recall)
##### Supplementary Findings and Exploration #####
|__04-SNAILS-tokenizer-analysis.ipynb              - Tokenizes SNAILS identifiers and explores their properties
|__05-SNAILS-token-naturalness-analysis.ipynb      - Exploring the alignment of tokens to natural language
|__06-SNAILS-naturalness-comparisons.ipynb         - SNAILS vs. Spider vs. Bird vs. SchemaPile naturalness
|__07-schemapile-naturalness.ipynb                 - ETL scripts for schemapile extraction and evaluation
|__08a-codes_query_execution_and_selection.ipynb   - Augmenting CodeS process to select first correct SQL
|__08b-DINSQL-CodeS-schema-subsetting-analysis.ipynb - Evaluating schema subsets generated by CodeS and DINSQL
|__09-spider-query-analysis.ipynb                  - Performance metric creation for Spider DEV (Native and modified) inference
```

Before running any of these notebooks, you will need to follow the project setup steps listed below.

## Natural View Building
One of the most pragmatic takeaways from our work on SNAILS is the idea of a natural view. Instead of adding additional information (e.g., column descriptions, example values, etc.) to a schema description in an LLM prompt, why don't we just make the target schema more natural? If you're building a new schema, that's relatively easy. If you're dealing with an already-existing schema, it's easier said then done.
For the case of existing schemas, we offer a view building pipeline that incorporates our classifier
and schema renamer.


### SNAILS Natural View Creation Workflow:

You will need to follow the steps in the README located in `SNAILS_Arifacts/naturalness_modifier/data_dict_reader` to get set up for your database.
Once the data_dict_reader has been configured, then run:

```bash
python ./classify_rename_and_build_view.py --database [your database name]
```

This will create a score file and a naturalness crosswalk file in ./schema_classifier_renamer_view_builder_output. Review the score file and make necessary corrections, then run:

```bash
python ./classify_rename_and_build_view.py --database [your database name] --build_view
```

This will create a .sql view file in the same directory. You can then run this file over your target
database to generate your natural views. Note that it assumes the presence of a db_nl schema, which you
should create prior to executing the view creation sql in your database.

If you want to try it out on a SNAILS database, run:

```bash
python ./classify_rename_and_build_view.py --database NYSED_SRC2022
```
and then
```bash
python ./classify_rename_and_build_view.py --database NYSED_SRC2022 --build_view
```
Which will create natural view .sql files for the SNAILS NYSED database using the NYSED PDF metadata and already-created fewshot prompt.

## SNAILS Project Setup

 This section describes the steps necessary to reproduce our findings. We recognize that they can be quite complex. If you have any questions or trouble with the steps below, please reach out to us!

### Dependencies
The dependency list is available in [requirements.txt](./requirements.txt). It is quite large, as we make use of several API services and libraries. We recommend using a virtual environment to install the dependencies.

#### Java Runtime
The SQL Parser can run using OpenJDK version 11 or greater

### Database Setup
Retrieve the .bak files from the project repository and restore them to a MS SQL Server RDBMS. We used Microsoft SQL Server Developer (64-bit); but it should work with any recent release of SQL Server.

Update the dbinfo.json file in the .local folder with your database connection information. Note that this requires the database to be accessible from the machine running the analysis using the username and password credentials provided.

### Accounts and API Keys
Some analysis relies on access to the OpenAI, TogetherAI, and Google Vertex APIs. API key information belongs in the .local folder and should be stored in .json format. Example files are provided in the .local folder.

#### Google Cloud
Use of the Codey and Google Vertex APIs require installation of the Google Cloud SDK and authentication with a Google Cloud account. See the [Google Cloud SDK documentation](https://cloud.google.com/sdk/docs/install) for installation instructions.

#### OpenAI
Modify ./local/openai.json with your OpenAI API key.

#### Phind-CodeLlama
Phind-CodeLlama inference relied on an endpoint hosted by TogetherAI which is no longer available. If you wish to reproduce NL-to-SQL inference using this model, a self-hosted solution will be required.

#### NL-to-SQL Experiment Quick Start
To rerun the NL-to-SQL experiments, you will need to, you may use the notebooks listed below. This does not include the identifier naturalness classification experiments--these are described in more detail later in this document. You can also find more detailed descriptions of these notebooks in the Experiment Notebooks section below.
 1. Generate NL-to-SQL and evaluate: run end-to-end-data-prep-and-prediction.py
 2. Evaluate tokens: tokenizer_analysis.ipynb
 3. Run query-level experiments: end-to-end-prototype-schema-and-query-analysis.ipynb
 4. Run identifier-level tokens: identifier-analysis.ipynb


## Real-World Database Collections and Question-Query Pairs

### Data Sources
Data was retrieved from multiple public-facing data repositories.
The nine selected databases represent data from scientific observations, crash sampling data, school system evaluations, and an enterprise resource planning system. All databases were migrated from their source formats into a MS SQL database.

#### MS SQL BAK Files:
The easiest way to access the databases is to download the .bak files and restore them to a MS SQL Server RDBMS.
For all databases, we offer compressed (.zip) MS SQL .bak files that can be imported into a MS SQL Server RDBMS. We used Microsoft SQL Server Developer (64-bit); but it should work with any recent release of SQL Server.

[Download all of the available MS SQL database .bak files here](https://drive.google.com/file/d/1EMQmdNx-a20TfZSDdNkFPLmXVS48IB2b/view?usp=drive_link). Place the SNAILS_database_collection.tar.gz file in the ```./SNAILS_Artifacts/databases/``` directory.

#### Sqlite Files:
We also ported the SNAILS collection to Sqlite. You can access the Sqlite versions of the [SNAILS MS SQL databases here](https://drive.google.com/file/d/16vgE_KCZMdKDKHtXgIQhvWdwO7ufKcjz/view?usp=drive_link).

#### Database Names and Data Source References:

- NPS IRMA Portal. https://irma.nps.gov/Portal/. Accessed: April 2023.

- **ASIS_20161108_HerpInv_Database** Robert Cook. 2016. Field Data for Assateague Island National Seashore Amphibian and Reptile Inventory. https://irma.nps.gov/DataStore/Reference/Profile/2236826. Accessed: April 2023.

- **ATBI** Thomas Evans. 2015. Great Smoky Mountains All Taxa Biodiversity Inventory (ATBI) Plot Vegetation Monitoring Database. https://irma.nps.gov/DataStore/Reference/Profile/2221324. Accessed: April 2023

- **KlamathInvasiveSpecies** Klamath Inventory and Monitoring Network. 2021. Exotic and Invasive Plants Monitoring Database. https://irma.nps.gov/DataStore/Reference/Profile/2288667. Accessed: April 2023.

- **PacificIslandLandbirds** Seth Judge and Kevin Kozar. 2023. Pacific Island Network Landbird Monitoring Dataset. https://irma.nps.gov/DataStore/Reference/Profile/2300107. https://doi.org/10.57830/2300107 Accessed: April 2023.

- **CratersWildlifeObservations** Charles Stefanic. 2021. Wildlife Observations Database: Craters of the Moon National Monument and Preserve 1921-2021. https://irma.nps.gov/DataStore/Reference/Profile/2192964. Accessed: April 2023.

- **NorthernPlainsFireManagement** Ian Muirhead. 2021. Northern Great Plains Fire Management: FFI Database.
https://irma.nps.gov/DataStore/Reference/Profile/2297267. Accessed: April 2023.

- **NTSB** National Center for Statistics and Analysis. 2022. Overview of the 2021 Crash Investigation Sampling System. https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/813397 Traffic Safety Facts Research Note. Report No. DOT HS 813 397.

- **NYSED_SRC2022** Report Card Database 2021-22. https://data.nysed.gov/files/essa/21-22/SRC2022.zip. Accessed: May 2023.

- **SBODemoUS** Marie-Laurence Poujois. 2021. Localized Demo Databases Now Available for SAP Business One 10.0 FP 2011. https://blogs.sap.com/2021/01/29/localized-demo-databases-now-available-for-sap-business-one-10.0-fp-2011/. Accessed: April 2023

### Natural Language Questions and Gold Queries
Question - query pairs are stored as executable .sql files with questions entered as SQL comments.
Each question is delinated by a ';' expression terminator.
In this repository, we [store them in a .zip file](./queries/NL-Questions-and-Gold-Queries.zip) to avoid inadvertent inclusion in future LLM training data sets.

**Example NL Question - Query Pair**
```
-- 5: How many events observed at least some stage of decay?
select count(distinct Event_ID) 
from tbl_Deadwood
;
```

#### Gold query profiling tool

In order to ensure a good usage of as many tables and columns as possible in each schema, we use a [Gold query writing tool notebook](./gold_query_writing_tool.ipynb) that parses each query to extract tables and columns, and compares this output to all tables and columns in the target database.
It gives a view of the tables and columns and indicates which ones have already been used, and which have not.
It also provides a histogram plot of the count of different clauses within each query to provide a visual understanding of the distribution of query complexity for each question set.


## Schema Naturalness Classification Dataset Generation
We made several different attempts at schema identifier naturalness classification including human scoring, heuristics-based, and machine learning.
Ultimately, we generated a ground truth dataset by validating and, where needed, modifying labels classified by a finetuned GPT davinci classification model.
Using this dataset, we then trained a local model based on Canine that outperforms the GPT-based model.

### Benchmark Identifier Naturalness Classification Gold data:

Gold classification category data used for analysis is located in the [gold-data](./gold-data/) folder.
You can refer to the [Gold Data readme](./gold-data/README.md) for specifics on each file.

### Classification Models

Model performance data is logged in the [auto-scoring](./classifier-inference-results) directory, and includes outputs and classification score calculations for the models described below. 

#### Classes
- N1: Regular
- N2: Low
- N3: Least

#### The Tagging Feature
If a model below is referred to as "tagged", it means that we employed a feature we call character tagging implemented in [tokenprocessing.py](./tokenprocessing.py). 


#### GPT 3.5 Turbo Fewshot Classification
No training was required for the fewshot classification approach, we simply use the OpenAI API and a pre-written completions prompt.

[The prompt](./prompts/fewshot-categoryexplanations.txt) |
[ml-naturalness-gpt-fewshot-classification.py](./ml-naturalness-gpt-fewshot-classification.py)

#### GPT Davinci Fine Tune

##### Training
Finetuning was accomplished using the OpenAI API training endpoint.

Example command:
```openai api fine_tunes.create -t .\manual-scoring\gpt-data\train_tagged_prepared.jsonl -m davinci --suffix "tagged_classifier"```

[Untagged Training Data](./manual-scoring/gpt-data/train_prepared.jsonl) |
[Tagged Training Data](./manual-scoring/gpt-data/train_tagged_prepared.jsonl) |
[Tagged Validation Data](./manual-scoring/gpt-data/validation_tagged_prepared.jsonl)
[Test Data](./manual-scoring/gpt-data/test-set.csv)

##### Inference
Inference is accomplished with:
[ml-naturalness-gpt-classification-finetuning.py](./ml-naturalness-gpt-classification-finetuning.py)

Note that this code relies upon the availability of an active openai account and a finetuned classifier model.

#### Canine

##### Training
Training is performed using: [snails_naturalness_classifier_training.py](./snails_naturalness_classifier_training.py) which is saved in the state that contains the optimal hyperparameters used for the best-performing generation 1 and 2 models.

We trained the models using an Nvidia GTX 1080 GPU with 8GiB of VRAM using transformers libraries, CUDA 12.1 and Torch 2.0.1.

Refer to requirements.txt for specific library versions.

##### Collection 1 data:
Generation 1 used the same human-generated training data as the davinci finetune in .csv format:
[Training Data](./manual-scoring/gpt-data/train.csv) |
[Validation Data](./manual-scoring/gpt-data/validation.csv) |
[Test Data](./manual-scoring/gpt-data/test-set.csv)

##### Collection 2 data:
Generation 2 was trained on an expanded set of training data that was generated by the davinci fine tune and curated by human researchers:
[Training Data](./manual-scoring/canine2/train.csv) |
[Test Data](./manual-scoring/canine2/test.csv) |
[Validation Data](./manual-scoring/canine2/validation.csv)

##### Inference (Collections 1 and 2)
Inference is accomplished with the CanineIdentifierClassifier class in [snails_naturalness_classifier.py](./snails_naturalness_classifier.py).

This class assumes the availability of a canine-based model trained using the steps described above.
The best-performing classifier is available on HuggingFace: https://huggingface.co/kyleluoma/SNAILS-word-naturalness-classifier


### Classification Heuristics
Our heuristics-based approach is called Word List Matching, and is implemented in 
[word-list-matching.py](word-list-matching.py)
Matching scores are generated by calculating using the following Heuristics:

## Schema Identifier Transformation and Naturalness-Modified Identifier Generation

To better observe the effect of naturalness levels on linking performance, we create alternate versions of all identifiers in the benchmark datasets.

Naturalness modification is performed the using [schemarenamer.py](schemarenamer.py) do_fewshot_identifier_transform function.

### More natural to less natural
Creating a less natural version of a natural identifier is a task of abbreviation. Less natural identifiers should contain elements of the full words that they represent.

in cases of transformation from higher to lower naturalness levels, do_fewshot_identifier_transform passes the fewshot prompts below to our callgpt.call_gpt function:

- [./prompts/fewshot-N1-to-N2.txt](./prompts/fewshot-N1-to-N2.txt)
- [./prompts/fewshot-N1-to-N3.txt](./prompts/fewshot-N1-to-N3.txt)
- [./prompts/fewshot-N2-to-N3.txt](./prompts/fewshot-N2-to-N3.txt)

### Less natural to more natural

Less natural to more natural transformation makes use of the program in the [data_dict_reader](./data_dict_reader/) subfolder. This program uses database metadata (xml, pdf, or csv format) and GPT fewshot prompting to expand abbreviated identifiers.

## Query Inference

Query inference and performance analysis is performed in the [end-to-end-prototype-data-prep-and-prediction](./end-to-end-prototype-data-prep-and-prediction.ipynb) notebook.

### Utilities

#### Query inference:
We call respective LLM APIs using:
- [call_llama.py](./call_llama.py)
- [callgpt.py](./callgpt.py)
- [callgooglenl.py](./callgooglenl.py)
- [calltogetherai.py](./calltogetherai.py)

We interact with target databases to generate prompts from system tables and extract resultsets using gold and predicted queries using [db_util.py](./db_util.py).

Queries require parsing for complexity analysis and identifier set comparisons.
The [query_profiler.py] QueryProfiler class interacts with our Antlr-based parser to extract the required data.

The [Java code and Antlr modified T-SQL and SQLITE grammar and lexer](./sql_parser_query_analyzer) are available at https://github.com/KyleLuoma/SQLParserQueryAnalyzer.
We also provide the [compiled binary .jar file ](./bin/SQLParserQueryAnalyzer_jar/SQLParserQueryAnalyzer.jar) which is required for parsing performed during query inference and performance evaluation.

## More on Performance Analysis 

This process starts with predicted and gold queries as the starting point and ends with a .xlsx-formatted file that contains the evaluation results described below.
Annotation files are in the [nl-to-sql_performance_annotations](./data/nl-to-sql_performance_annotations/) folder.

### Evaluation

The same [end-to-end-prototype-data-prep-and-prediction](./src/end-to-end-data-prep-and-prediction.py) script performs post-inference query analysis including:

- Result-set comparison
- Query string comparison
- Query identifier comparison (yields F1, Recall, Precision metrics) for schema linking evaluation

#### Result Set Comparison

We conduct set/superset evaluation using the compare_gold_to_generated function in [semantic_compare.py](./src/semantic_compare.py). 

#### Manual Match Comparison

Manual matching is facilitated by the [query_manual_evaluation.py](./src/query_manual_evaluation.py) tikinter-based Python GUI application.

#### Query Plan Comparison

Though we did not make use of query plan comparisons in our evaluation pipeline, we retain the process within the end-to-end-prototype-data-prep-and-prediction notebook.
Basically, this process involves executing queries over our target MS SQL databases using the --set showplan_xml on command. 
This yields an XML result that contains a string-formatted query plan.
We parse the xml to extract the query plan, and then perform string comparisons of gold and predicted query plans.

### Data Analysis

In the data analysis stage, we use the ConsolidatedResultsLoader class in [load_consolidated_results.py](./src/util/load_consolidated_results.py) which is an ETL tool that pulls in all of the performance annotation files and joins with additional analysis data which generates a single annotation dataframe which we use in our performance data analysis in the [02-SNAILS-NL-to-SQL-results-analysis.ipynb](./02-SNAILS-NL-to-SQL-results-analysis.ipynb) and [03-SNAILS-identifier-analysis.ipynb](./03-SNAILS-identifier-analysis.ipynb) notebooks.

We offer the full output of the load_consolidated_results.py file here:
[analysis-gold-data](./data/gold-data/analysis-gold-data-2024-01-18.xlsx) which you can use for your own analysis.



